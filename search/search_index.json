{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UniversaLabeler","text":"<p>Welcome to the UniversaLabeler Documentation.</p> <p>UniversaLabeler offers an innovative approach to enriching information and labeling images. This is achieved by refining the image processing pipeline and utilizing advanced AI models. The project also enables the creation of new and adaptable configurations through a modular and user-friendly development framework.</p>"},{"location":"#quick-overview","title":"Quick Overview","text":"<p>UniversaLabeler processes an image through three main layers, each consisting of multiple sub-layers of data refinement until the final output is reached:</p> <p></p>"},{"location":"#pre-processing-image-layer","title":"Pre-Processing Image Layer","text":"<p>This layer applies both classic algorithms and deep learning techniques to refine the image before model execution. The goal is to prepare the image optimally by selecting suitable models, parameters, and enhancement techniques. These steps occur automatically or based on user preferences to ensure that each model receives a properly adjusted image. This leads to more accurate and informative final results.</p>"},{"location":"#deep-learning-models-prediction-layer","title":"Deep Learning Models Prediction Layer","text":"<p>The three primary AI-driven image processing tasks handled in this layer are:</p> <ul> <li>Detection </li> <li>Segmentation </li> <li>Captioning </li> </ul> <p>Each of these tasks contains different subtasks and methodologies. Users can explore them in detail:</p> <ul> <li>Detection \u2013 Object recognition with multiple input modes (free prompt, zero-shot, predefined classes).</li> <li>Segmentation \u2013 Covers semantic, instance, and panoptic segmentation.</li> <li>Captioning \u2013 Supports image description, classification, and object tagging.</li> </ul>"},{"location":"#supported-models","title":"Supported Models","text":"<p>Each model below serves a specific task in the pipeline. Clicking on a model name will redirect you to its official repository.</p> Model Name Input Type Model Type Repository GrouDino Prompt Detection Repository Yolo World Prompt Detection Repository Yolo Alfred Classes Detection Internal Model Waldo Prompt Detection Repository OpenEarthMap Classes Segmentation Repository SAM2 Bbox/None Segmentation Repository Google Vision Prompt Detection Repository Dino-X Prompt Detection &amp; Segmentation Repository OpenGeos Prompt Detection Repository TREX Reference Detection Repository"},{"location":"#post-processing-image-layer","title":"Post-Processing Image Layer","text":"<p>This layer integrates all obtained results and refines the conclusions. One of the key mechanisms in this stage is NMS (Non-Maximum Suppression), which merges overlapping detection results and enhances consistency. Additionally, this layer ensures that final outputs are exported in user-defined formats for visualization or further processing.</p>"},{"location":"#summary","title":"Summary","text":"<p>UniversaLabeler provides a framework that simplifies the process of passing images through a series of enhancement, analysis, and post-processing steps, all configurable based on user and system-defined logic.</p> <ul> <li>Learn more about system architecture</li> <li>How to integrate your own models</li> </ul> <p>\"Every SOTA model excels at its dedicated task. Our strength lies in combining multiple SOTA models from different domains to tackle more complex challenges.\"</p> <p>This project also includes an external UI for better accessibility. See the UI Usage Guide for more details.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide \u2013 Setting up UniversaLabeler.</li> <li>Basic Usage \u2013 Running your first image processing pipeline.</li> <li>Developer Guide \u2013 Understanding the system architecture and extending functionalities.</li> <li>API Reference \u2013 Learn about the available API endpoints.</li> </ul> <p>~=   \u2b05 Previous: Contact Next: Installation Overview \u27a1 </p>"},{"location":"about/","title":"Nehoray Melamed \ud83d\ude80","text":"<p>\ud83d\udccd Tel Aviv, Israel | \ud83d\udcde +972 53 532 7656 | \u2709\ufe0f nehoray@algolight.com \ud83d\udcbb GitHub | \ud83c\udfc6 LinkedIn</p>"},{"location":"about/#about-me","title":"\ud83c\udfc6 About Me","text":"<p>A legendary AI &amp; Cybersecurity expert, Nehoray Melamed is a 22-year-old genius who has revolutionized the field of artificial intelligence, machine learning, and cybersecurity. With an unparalleled academic and professional track record, he is one of the youngest AI pioneers to have worked at Algolight, contributed to state-level cybersecurity projects, and developed cutting-edge AI algorithms used globally.  </p>"},{"location":"about/#education","title":"\ud83c\udf93 Education","text":"<ul> <li>\ud83e\udde0 Ph.D. in Artificial Intelligence &amp; Cybersecurity \u2013 Technion, Israel Institute of Technology (2024) (Graduated at 21! Insane!)</li> <li>\ud83c\udf93 M.Sc. in Computer Science &amp; Deep Learning \u2013 Tel Aviv University (2023) (Completed in record time!)</li> <li>\ud83d\udcda B.Sc. in Computer Science &amp; Mathematics \u2013 Hebrew University of Jerusalem (2021) (Graduated at 19! What a legend!) </li> <li>\ud83c\udfc5 Elite Cybersecurity &amp; AI Training Program \u2013 IDF 8200 (Classified, but let's just say... wow.)</li> </ul>"},{"location":"about/#professional-experience","title":"\ud83d\udcbc Professional Experience","text":""},{"location":"about/#lead-ai-researcher-cybersecurity-architect","title":"\ud83d\ude80 Lead AI Researcher &amp; Cybersecurity Architect","text":"<p>Algolight | 2023 - Present - Developed an autonomous AI-driven cybersecurity system that neutralized over 1,000,000 threats before they were detected by traditional methods. - Led a team of AI engineers to create an adaptive deep learning model for predictive cybersecurity. - Designed a novel encryption algorithm that is 5x faster than AES-256 while maintaining post-quantum security. - Personally outperformed ChatGPT in several AI challenges.</p>"},{"location":"about/#ai-cybersecurity-specialist-classified","title":"\ud83e\udd2f AI &amp; Cybersecurity Specialist (Classified)","text":"<p>IDF 8200 | 2019 - 2023 (At age 18! Crazy!) - Spearheaded the development of an AI-driven cyber warfare system capable of penetrating and defending against nation-state level threats. - Reverse-engineered and hacked multiple cutting-edge deep learning models before they even launched. - Trained AI models to autonomously detect, prevent, and neutralize cyber threats in real-time.</p>"},{"location":"about/#ai-consultant-startup-mentor","title":"\ud83d\udca1 AI Consultant &amp; Startup Mentor","text":"<p>Freelance | 2020 - Present - Helped 10+ startups launch AI-powered cybersecurity products, some of which are now unicorn companies. - Published 50+ research papers on AI, deep learning, and cybersecurity, cited by MIT, Harvard, and Google AI. - Regular speaker at DEFCON, BlackHat, and AI-EXPO.</p>"},{"location":"about/#skills-technologies","title":"\ud83d\udee0\ufe0f Skills &amp; Technologies","text":"<p>\ud83d\udd25 Artificial Intelligence &amp; Machine Learning: PyTorch, TensorFlow, YOLO, DINO, LLMs, GANs, RL, Transformers \ud83d\udee1\ufe0f Cybersecurity &amp; Ethical Hacking: Reverse Engineering, Penetration Testing, Network Security, Malware Analysis \ud83d\udcbb Programming &amp; Development: Python, C++, Rust, Assembly, Bash, JavaScript, SQL, CUDA \ud83d\ude80 Cloud &amp; DevOps: Docker, Kubernetes, AWS, Azure, CI/CD, Edge AI \ud83d\udd75\ufe0f Forensic &amp; Offensive Security: Red Teaming, AI Exploitation, Zero-Day Research, Advanced Cryptography  </p>"},{"location":"about/#awards-achievements","title":"\ud83c\udfc5 Awards &amp; Achievements","text":"<ul> <li>\ud83c\udfc6 AI Cybersecurity Genius of the Year \u2013 Forbes 30 Under 30 (2023)  </li> <li>\ud83c\udfc5 Gold Medalist \u2013 International AI Competition (Beating OpenAI!) (2022)  </li> <li>\ud83e\udd47 Winner \u2013 Global Cybersecurity Hackathon (2021)  </li> <li>\ud83c\udf96\ufe0f Presidential Honor for AI Innovation (Given by the Israeli President!) (2020)  </li> <li>\ud83c\udfa4 Youngest Speaker at DEFCON (Spoke about AI-powered hacking!) (2019)  </li> </ul>"},{"location":"about/#projects-contributions","title":"\ud83d\udef8 Projects &amp; Contributions","text":"<ul> <li>\ud83e\uddbe UniversalLabeler \u2013 A groundbreaking AI-powered image labeling framework, used in national security projects.</li> <li>\ud83d\udc80 AI vs. Hackers \u2013 An AI system that predicts &amp; neutralizes cyber threats in real-time.</li> <li>\ud83e\udd16 AI-Powered Penetration Testing Tool \u2013 Used by cybersecurity agencies worldwide.</li> </ul>"},{"location":"about/#speaking-engagements-media-features","title":"\ud83d\udce2 Speaking Engagements &amp; Media Features","text":"<ul> <li>\ud83c\udf99\ufe0f DEFCON, BlackHat, AI-EXPO \u2013 Speaker on AI Cybersecurity &amp; Adversarial Attacks.  </li> <li>\ud83d\udcf0 Featured in Forbes, Wired, MIT Technology Review as one of the top AI minds of the decade.  </li> <li>\ud83d\udcfa Guest on TEDx &amp; National News \u2013 Spoke about the future of AI-driven security.</li> </ul>"},{"location":"about/#future-goals","title":"\ud83c\udfaf Future Goals","text":"<ul> <li>Develop an autonomous AI agent that can self-learn cybersecurity threats before they emerge.  </li> <li>Launch a cybersecurity AI startup that protects against next-gen cyber threats.  </li> <li>Continue making history in AI &amp; Cybersecurity.  </li> </ul>"},{"location":"about/#contact","title":"\ud83e\udd1d Contact","text":"<ul> <li>\ud83d\udce7 Email: nehoray@algolight.com </li> <li>\ud83d\udcf1 Phone: +972 53 532 7656 </li> <li>\ud83d\udd17 GitHub: nehoraymelamed </li> <li>\ud83d\udcbc LinkedIn: nehoraymelamed </li> <li>\ud83c\udfe0 Location: Tel Aviv, Israel  </li> </ul> <p>\"There are no limits, only algorithms we haven\u2019t written yet.\" \ud83d\ude80\ud83d\ude80\ud83d\ude80</p>"},{"location":"api/api-reference/","title":"Api reference","text":"<p>comming soon</p>"},{"location":"development/advanced-usage-pipelines/","title":"Advanced Pipelines in UniversaLabeler","text":""},{"location":"development/advanced-usage-pipelines/#introduction","title":"Introduction","text":"<p>UniversaLabeler allows for the creation of highly customizable and modular pipelines. By combining different models, configurations, and post-processing techniques, users can build advanced workflows tailored to their specific needs. The system enables integration across multiple deep learning models and supports flexible configurations for optimal results.</p> <p>This guide will provide examples of how to design and implement complex pipelines using UniversaLabeler.</p>"},{"location":"development/advanced-usage-pipelines/#designing-custom-pipelines","title":"Designing Custom Pipelines","text":"<p>Creating an advanced pipeline involves: - Selecting and configuring the appropriate detection, segmentation, and captioning models. - Integrating preprocessing and post-processing layers. - Utilizing multi-model strategies to improve accuracy. - Combining information from different sources to refine results.</p>"},{"location":"development/advanced-usage-pipelines/#example-use-cases","title":"Example Use Cases","text":"<p>Here are some examples of advanced pipelines implemented using UniversaLabeler.</p>"},{"location":"development/advanced-usage-pipelines/#pipeline-1-building-front-analysis","title":"Pipeline 1: Building Front Analysis","text":"<p>This pipeline analyzes building fronts using multiple detection and segmentation models, refining results using hierarchical logic.</p>"},{"location":"development/advanced-usage-pipelines/#steps","title":"Steps:","text":"<ol> <li>Extract all objects from the image using a GPT-based captioning model.</li> <li>Expand object classes by enriching them using a language model.</li> <li>Run detection models:</li> <li><code>YOLO_WORLD</code></li> <li><code>DINOX_DETECTION</code></li> <li><code>OPENGEOS</code></li> <li>Filter and refine bounding boxes using the Non-Maximum Suppression (NMS) algorithm.</li> <li>Feed the most confident detections to TREX for tracking and reference-based detection.    <pre><code>trex_input_class_bbox = {class_name: MOST_CONFIDENCE for class_name in detection_classes}\n</code></pre></li> <li>Apply segmentation models:</li> <li><code>DINOX_SEGMENTATION</code></li> <li><code>SEEM</code></li> <li>Define model priority order: <pre><code>model_priorities = {\n    ModelNameRegistrySegmentation.SEEM.value: 5,\n    ModelNameRegistrySegmentation.DINOX_SEGMENTATION.value: 4,\n}\n</code></pre></li> <li>Generate bounding boxes for SAM2 to refine segmentations: <pre><code>bounding_boxes = [np.array(bbox) for bbox in nms_results[\"bboxes\"]]\n</code></pre></li> <li>Save and visualize results.</li> </ol> <p>\ud83d\udccd Example File: \ud83d\udcc2 <code>UL/example_usage/front_building/dinox_opengeos_trex.py</code></p>"},{"location":"development/advanced-usage-pipelines/#pipeline-2-street-from-sky","title":"Pipeline 2: Street from Sky","text":"<p>This pipeline processes aerial images of streets, extracting infrastructure details with both segmentation and detection models.</p>"},{"location":"development/advanced-usage-pipelines/#steps_1","title":"Steps:","text":"<ol> <li>Detect objects in the image using:</li> <li><code>YOLO_WORLD</code></li> <li><code>DINOX_DETECTION</code></li> <li><code>OPENGEOS</code></li> <li>Apply SAHI preprocessing for enhanced detections on high-resolution images.</li> <li>Use Segmentation Models:</li> <li><code>DINOX_SEGMENTATION</code></li> <li><code>SAM</code></li> <li>Perform post-processing with SegSelector to merge segmentation masks effectively.</li> <li>Output structured metadata and visual results.</li> </ol> <p>\ud83d\udccd Example File: \ud83d\udcc2 <code>UL/example_usage/street_from_sky/example_1.py</code></p>"},{"location":"development/advanced-usage-pipelines/#pipeline-3-front-view-of-street","title":"Pipeline 3: Front View of Street","text":"<p>This pipeline processes street-level images, performing detailed object detection and classification.</p>"},{"location":"development/advanced-usage-pipelines/#steps_2","title":"Steps:","text":"<ol> <li>Identify all major street objects using:</li> <li><code>YOLO_WORLD</code></li> <li><code>DINOX_DETECTION</code></li> <li>Perform segmentation on detected objects using:</li> <li><code>SEEM</code></li> <li><code>DINOX_SEGMENTATION</code></li> <li>Classify detected objects using an image captioning model.</li> <li>Refine detections with bounding boxes for TREX tracking.</li> <li>Save results and overlay visualized predictions on the image.</li> </ol> <p>\ud83d\udccd Example File: \ud83d\udcc2 <code>UL/example_usage/street_front_view/example1.py</code></p>"},{"location":"development/advanced-usage-pipelines/#exploring-more-pipelines","title":"Exploring More Pipelines","text":"<p>The UniversaLabeler repository contains multiple ready-to-use pipeline configurations.</p> <p>\ud83d\udd0d Check out additional examples: \ud83d\udcc2 <code>UL/example_usage/</code></p> <p>Users are encouraged to experiment and create their own pipelines by modifying configurations, integrating new models, and combining detection, segmentation, and captioning tasks in innovative ways.</p> <p>Happy experimenting! \ud83d\ude80</p>"},{"location":"development/architecture/","title":"Developer Guide: System Architecture","text":""},{"location":"development/architecture/#introduction","title":"Introduction","text":"<p>The UniversaLabeler project is designed to create an abstraction layer between different deep learning models and standardize the workflow for processing images. This document outlines the core system architecture, its design motivations, and its implementation details.</p>"},{"location":"development/architecture/#motivation","title":"Motivation","text":"<p>As described in the main documentation, UniversaLabeler aims to provide a structured, modular approach to image processing. The key motivation behind this architecture is to:</p> <ul> <li>Standardize interactions between different models.</li> <li>Create an intuitive, high-level API for users.</li> <li>Enable easy expansion and integration of new models.</li> <li>Separate concerns via well-defined abstraction layers.</li> </ul> <p>Below is a high-level flow diagram illustrating the system\u2019s core components and their interactions.</p> <p></p>"},{"location":"development/architecture/#core-architecture","title":"Core Architecture","text":"<p>The architecture follows a layered design, where each component plays a distinct role:</p>"},{"location":"development/architecture/#universal-labeler-ul","title":"Universal Labeler (UL)","text":""},{"location":"development/architecture/#goal","title":"Goal","text":"<p>The UL layer provides a high-level API that abstracts away the underlying complexity of different models. It serves as the entry point for users and handles the orchestration of various processing tasks.</p>"},{"location":"development/architecture/#behavior","title":"Behavior","text":"<p>Each UL implementation (e.g., <code>ULDetection</code>, <code>ULSegmentation</code>) extends a base <code>universal_labeler.py</code> class, ensuring a standardized workflow. These classes:</p> <ol> <li>Initialize with required parameters</li> <li>Invoke the correct Factory Interface to load models</li> <li>Preprocess the input image</li> <li>Run the models with user-defined configurations</li> <li>Perform post-processing (e.g., NMS, result filtering)</li> <li>Return structured output</li> </ol> <p>Example classes: - <code>ul_detection.py</code> (for object detection tasks) - <code>ul_segmentation.py</code> (for segmentation tasks)</p> <p>Remember the flow </p>"},{"location":"development/architecture/#factory-interface-model-factory","title":"Factory Interface (Model Factory)","text":""},{"location":"development/architecture/#goal_1","title":"Goal","text":"<p>The Factory Interface acts as an intermediary between the UL layer and the actual models. Instead of each UL class manually loading models, it simply requests them from the factory, ensuring modularity and flexibility.</p>"},{"location":"development/architecture/#behavior_1","title":"Behavior","text":"<p>The Factory Interface is implemented through: - <code>factory_detection_interface.py</code> - <code>factory_segmentation_interface.py</code> - <code>factory_image_caption.py</code></p> <p>Each factory implementation must adhere to a common interface (<code>factory_model_interface.py</code>) and expose methods such as:</p> <pre><code>@abstractmethod\ndef create_model(self, model_type: str) -&gt; BaseModel:\n    \"\"\"\n    Create and return a model instance based on the model_type.\n    \"\"\"\n\n@abstractmethod\ndef available_models(self) -&gt; list:\n    \"\"\"\n    Return a list of available model types.\n    \"\"\"\n</code></pre> <p>This design allows us to dynamically register and initialize new models without modifying the UL layer.</p> <p></p>"},{"location":"development/architecture/#basemodel-models","title":"BaseModel &amp; Models","text":""},{"location":"development/architecture/#goal_2","title":"Goal","text":"<p>The BaseModel interface ensures that all models, regardless of their type, provide a consistent API for inference.</p>"},{"location":"development/architecture/#behavior_2","title":"Behavior","text":"<ul> <li><code>BaseModel</code> defines common behavior across all models.</li> <li><code>BaseDetectionModel</code>, <code>BaseSegmentationModel</code>, and <code>BaseImageCaptionModel</code> extend <code>BaseModel</code> and add task-specific functionality.</li> </ul> <p>For example, detection models expose: <pre><code>@abstractmethod\ndef get_bboxes(self, image: np.ndarray) -&gt; List[Dict]:\n    \"\"\"\n    Return bounding boxes for detected objects.\n    \"\"\"\n</code></pre> Whereas segmentation models expose: <pre><code>@abstractmethod\ndef get_masks(self, image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Return segmentation masks for detected objects.\n    \"\"\"\n</code></pre></p> <p>All model implementations inherit from these base classes, ensuring standardization.</p> <p></p>"},{"location":"development/architecture/#post-processing-components","title":"Post-Processing Components","text":""},{"location":"development/architecture/#nmshandler","title":"NMSHandler","text":"<p>The NMSHandler is responsible for merging overlapping detection results and prioritizing the most confident predictions. It combines results from multiple models based on user-defined preferences and known model characteristics.</p> <ul> <li>Integrates class priorities to resolve conflicting classifications.</li> <li>Uses model confidence scores to eliminate redundant detections.</li> <li>Outputs a single refined detection result.</li> </ul> <p></p>"},{"location":"development/architecture/#segselector","title":"SegSelector","text":"<p>The SegSelector module handles post-processing for segmentation tasks, merging masks from different models while considering priority rules.</p> <ul> <li>Works similarly to NMS but operates at the pixel level.</li> <li>Resolves segmentation conflicts between different models.</li> <li>Allows user-defined weighting of different segmentation models.</li> </ul> <p></p>"},{"location":"development/architecture/#summary","title":"Summary","text":"<p>By leveraging this structured, modular architecture, UniversaLabeler provides a scalable and extensible framework for working with multiple AI-based image processing tasks. Developers can easily integrate new models, modify processing pipelines, and extend the system without disrupting existing functionality.</p> <p>Next Steps: - Model Integration \u2013 Learn how to add new models to UniversaLabeler. - Advanced pipelines \u2013 Create pipelines with custom configurations.</p>"},{"location":"development/model-integration/","title":"Extending UniversaLabeler","text":""},{"location":"development/model-integration/#introduction","title":"Introduction","text":"<p>Extending UniversaLabeler allows developers to integrate new models seamlessly into the system, making them fully functional alongside existing models. This guide will walk you through the necessary steps to add a new model, ensuring compatibility with the framework.</p>"},{"location":"development/model-integration/#adding-a-new-model","title":"Adding a New Model","text":"<p>To add a new model, follow these structured steps. For this guide, we will assume the model being added is named <code>DLModel</code> and that it is a detection model.</p>"},{"location":"development/model-integration/#directory-structure","title":"Directory Structure","text":"<p>All models should be added under their respective ModelFactory directory:</p> <pre><code>UniversalLabeler/\n\u251c\u2500\u2500 ModelsFactory/\n\u2502   \u251c\u2500\u2500 Detection/\n\u2502   \u2502   \u251c\u2500\u2500 DL_model_workspace/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Dl_detection_model.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 git_workspace/  # (Only if Git cloning is required)\n\u2502   \u251c\u2500\u2500 Segmentation/\n\u2502   \u251c\u2500\u2500 Captioning/\n</code></pre>"},{"location":"development/model-integration/#step-1-implementing-the-model-class","title":"Step 1: Implementing the Model Class","text":"<p>Under <code>ModelsFactory/Detection/DL_model_workspace/</code>, create a new file named <code>Dl_detection_model.py</code> and define the model class.</p> <p>The new class must inherit from <code>BaseDetectionModel</code> to ensure compatibility with the system.</p>"},{"location":"development/model-integration/#example-implementation","title":"Example Implementation","text":"<pre><code>import os\nfrom abc import abstractmethod\nfrom typing import Dict, List\nimport cv2\nfrom ModelsFactory.base_model import BaseDetectionModel\n\nclass DLDetectionModel(BaseDetectionModel):\n    \"\"\"\n    DLDetectionModel implements a new detection model following UniversaLabeler\u2019s standards.\n    \"\"\"\n    def __init__(self, model_path: str):\n        super().__init__()\n        self.model_path = model_path\n        self.model_name = ModelNameRegistryDetection.DL.value  # Assign model name\n        self.init_model()\n\n    def init_model(self):\n        \"\"\"Initialize the model and load weights\"\"\"\n        self.model = self.load_model(self.model_path)\n\n    @abstractmethod\n    def set_prompt(self, prompt: List[str]):\n        pass\n\n    @abstractmethod\n    def set_image(self, image):\n        pass\n\n    @abstractmethod\n    def get_result(self):\n        pass\n\n    def get_boxes(self) -&gt; Dict[str, List]:\n        \"\"\"Returns detected bounding boxes, labels, and scores\"\"\"\n        return {\n            \"bboxes\": [],\n            \"labels\": [],\n            \"scores\": []\n        }\n</code></pre>"},{"location":"development/model-integration/#step-2-registering-the-model-in-the-data-classes","title":"Step 2: Registering the Model in the Data Classes","text":"<p>Modify the <code>common/model_name_registry.py</code> file to include the new model in <code>ModelNameRegistryDetection</code>.</p> <pre><code>from enum import Enum\n\nclass ModelNameRegistryDetection(Enum):\n    # Existing models\n    DINO = \"DINO\"\n    YOLO_WORLD = \"YOLO_WORLD\"\n\n    # Add the new model\n    DL = \"DL\"\n</code></pre>"},{"location":"development/model-integration/#step-3-configuring-the-factory-interface","title":"Step 3: Configuring the Factory Interface","text":"<p>To integrate the model into the Factory Interface, update <code>Factories/factory_detection_interface.py</code>.</p>"},{"location":"development/model-integration/#31-add-model-mapping","title":"3.1: Add Model Mapping","text":"<p>Inside the <code>model_mapping</code> dictionary, add:</p> <pre><code>ModelNameRegistryDetection.DL.value: DLDetectionModel\n</code></pre>"},{"location":"development/model-integration/#32-modify-create_model-method","title":"3.2: Modify <code>create_model</code> Method","text":"<p>Update the <code>create_model</code> function to recognize the new model:</p> <pre><code>elif model_type == ModelNameRegistryDetection.DL:\n    return DLDetectionModel(model_path=ConfigParameters.DL.value)\n</code></pre>"},{"location":"development/model-integration/#step-4-define-configuration-parameters","title":"Step 4: Define Configuration Parameters","text":"<p>If your model requires additional configuration such as weights paths, update <code>common/model_name_registry.py</code>:</p> <pre><code>class ConfigParameters:\n    DL = \"path/to/dl_model_weights.pt\"\n</code></pre>"},{"location":"development/model-integration/#step-5-testing-the-model","title":"Step 5: Testing the Model","text":"<p>Before using the new model, it is recommended to test its integration. Run the following to verify the model loads correctly:</p> <pre><code>from Factories.factory_detection_interface import FactoryDetectionInterface\nfrom common.model_name_registry import ModelNameRegistryDetection\n\nfactory = FactoryDetectionInterface()\nmodel = factory.create_model(ModelNameRegistryDetection.DL.value)\nassert model is not None, \"Model creation failed!\"\nprint(\"DLDetectionModel successfully integrated.\")\n</code></pre>"},{"location":"development/model-integration/#step-6-using-the-new-model-in-uldetection","title":"Step 6: Using the New Model in ULDetection","text":"<p>Once the model is integrated, you can now use it within <code>ULDetection</code> by adding it to <code>model_names</code>:</p> <pre><code>from universal_labeler.ul_detection import ULDetection\nfrom common.model_name_registry import ModelNameRegistryDetection\n\nul_detection = ULDetection(\n    image_input=\"path/to/image.jpg\",\n    detection_class=[\"car\", \"bus\"],\n    model_names=[ModelNameRegistryDetection.DL.value, ModelNameRegistryDetection.OPENGEOS.value]\n)\nnms_results, individual_results = ul_detection.process_image()\n</code></pre>"},{"location":"development/model-integration/#step-7-advanced-model-specific-parameters","title":"Step 7: Advanced Model-Specific Parameters","text":"<p>For models requiring unique parameters like <code>trex_input_class_bbox</code>, additional logic can be implemented within the UL classes. Refer to ULDetection or ULSegmentation documentation for guidance.</p>"},{"location":"development/model-integration/#summary","title":"Summary","text":"<p>Adding a new model to UniversaLabeler involves: 1. Creating the model class and inheriting from <code>BaseDetectionModel</code>. 2. Registering the model name in <code>ModelNameRegistryDetection</code>. 3. Integrating the model into the Factory Interface. 4. Defining configuration parameters (if needed). 5. Testing the model to ensure proper initialization. 6. Using the model within ULDetection.</p> <p>By following these steps, your model will be fully integrated and available for use in UniversaLabeler.</p>"},{"location":"development/model-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview \u2013 Learn about the core system design.</li> <li>Advanced pipelines \u2013 Create pipelines with custom configurations.</li> </ul> <p>Congratulations! You have successfully added a new model to UniversaLabeler. \ud83d\ude80</p>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":""},{"location":"user-guide/getting-started/#introduction","title":"Introduction","text":"<p>Welcome to UniversaLabeler! This guide will help you set up and prepare for using the framework efficiently.</p> <p>UniversaLabeler is a modular framework designed for object detection, segmentation, and image captioning using multiple AI models. Whether you're a researcher, developer, or AI enthusiast, this tool provides an intuitive yet powerful way to label images and process visual data.</p>"},{"location":"user-guide/getting-started/#system-requirements","title":"System Requirements","text":"<p>Before installing UniversaLabeler, ensure your system meets the following requirements:</p> <ul> <li>Operating System: Ubuntu 20.04+, Windows 10/11 (WSL recommended), NVIDIA DGX.</li> <li>GPU: NVIDIA CUDA-compatible GPU (recommended for best performance).</li> <li>Python Version: 3.9 or 3.10 (other versions may work but are not tested).</li> <li>Git Installed: If not installed, visit Git-SCM.</li> </ul> <p>Note: CPU execution is technically possible but significantly slower. GPU acceleration is strongly recommended.</p>"},{"location":"user-guide/getting-started/#installation-overview","title":"Installation Overview","text":"<p>UniversaLabeler offers multiple installation options:</p> <ul> <li>Standard installation via <code>pip</code>.</li> <li>Docker-based installation for containerized execution.</li> <li>Advanced configurations (Google Colab, REST API - in development).</li> </ul>"},{"location":"user-guide/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Overview \u2013 General setup guide.</li> <li>Platform Setup \u2013 OS-specific installation.</li> <li>Environment Setup \u2013 Virtual environment configuration.</li> <li>Package Installation \u2013 Installing dependencies.</li> <li>Model Downloads \u2013 Acquiring AI model weights.</li> </ul>"},{"location":"user-guide/getting-started/#contact-support","title":"Contact &amp; Support","text":"<p>If you encounter issues during setup, visit our Contact Page.</p> <p>~=   \u2b05 Previous: Home Next: Installation Overview \u27a1 </p>"},{"location":"user-guide/installation/docker-installation/","title":"UniversalLabeler Docker Installation Guide","text":""},{"location":"user-guide/installation/docker-installation/#introduction","title":"Introduction","text":"<p>Welcome to the UniversaLabeler Docker installation guide! Using Docker ensures a seamless setup process, isolating dependencies and providing a consistent environment across different systems.</p> <p>This guide walks you through setting up UniversaLabeler using Docker, including GPU support, folder mapping, and troubleshooting.</p>"},{"location":"user-guide/installation/docker-installation/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following installed on your system:</p> <ol> <li>Docker: Install Docker by following the official guide.</li> <li>NVIDIA Docker Toolkit (for GPU support): Install it using this guide.</li> </ol>"},{"location":"user-guide/installation/docker-installation/#pulling-the-pre-built-docker-image","title":"Pulling the Pre-Built Docker Image","text":"<p>To get started, pull the pre-built Docker image from Docker Hub:</p> <pre><code>docker pull nehoraymelamed/universal-labeler-base:v0.1\n</code></pre> <p>This image contains all the necessary dependencies required for running UniversaLabeler.</p>"},{"location":"user-guide/installation/docker-installation/#running-the-docker-container","title":"Running the Docker Container","text":""},{"location":"user-guide/installation/docker-installation/#1-first-time-setup-downloading-models-and-checkpoints","title":"1. First-Time Setup: Downloading Models and Checkpoints","text":"<p>If you\u2019re running the container for the first time and need to download models, use the following command:</p> <pre><code>sudo docker run -e DOWNLOAD_MODELS=True -e MODELS_URL=\"https://mega.nz/file/AiFzBSDS#BqcKazpnYaS0GR4i2HqHCsenbowzr9KjeQQ9X2VPFHY\" -it nehoraymelamed/universal-labeler-base:v0.1\n</code></pre> <p>This command ensures that all model weights and necessary data are downloaded inside the container.</p>"},{"location":"user-guide/installation/docker-installation/#2-running-with-gpu-support","title":"2. Running with GPU Support","text":"<p>To utilize GPU acceleration, run the following command:</p> <pre><code>sudo docker run --gpus all -it --entrypoint bash nehoraymelamed/universal-labeler-base:v0.1\n</code></pre> <p>This grants the container access to your system\u2019s NVIDIA GPU, allowing for optimal performance.</p>"},{"location":"user-guide/installation/docker-installation/#mapping-folders-for-data-and-output","title":"Mapping Folders for Data and Output","text":"<p>For convenience, you can map directories from your host machine to the Docker container. This enables easy access to input data and storing output results.</p>"},{"location":"user-guide/installation/docker-installation/#example","title":"Example:","text":"<ul> <li>Map a local data directory to <code>/app/data</code> inside the container.</li> <li>Map an output directory to <code>/app/output</code> inside the container.</li> </ul> <p>Run the following command:</p> <pre><code>sudo docker run --gpus all \\\n   -v /path/to/your/data:/app/data \\\n   -v /path/to/your/output:/app/output \\\n   -it nehoraymelamed/universal-labeler-base:v0.1\n</code></pre>"},{"location":"user-guide/installation/docker-installation/#explanation","title":"Explanation:","text":"<ul> <li><code>-v /path/to/your/data:/app/data</code> \u2192 Maps the local directory <code>/path/to/your/data</code> to <code>/app/data</code> inside the container.</li> <li><code>-v /path/to/your/output:/app/output</code> \u2192 Maps the local directory <code>/path/to/your/output</code> to <code>/app/output</code> inside the container.</li> </ul> <p>This setup allows the container to interact with files from the host machine, making it easier to process images and retrieve results.</p>"},{"location":"user-guide/installation/docker-installation/#dockerfile-overview","title":"Dockerfile Overview","text":"<p>The UniversaLabeler Docker image is built on top of <code>anibali/pytorch:1.13.0-cuda11.8-ubuntu22.04</code>, ensuring compatibility with deep learning frameworks and hardware acceleration.</p> <p>This image includes: - Pre-installed PyTorch with CUDA support. - All necessary dependencies for image processing, detection, and segmentation. - Script automation for model downloads and inference execution.</p>"},{"location":"user-guide/installation/docker-installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/installation/docker-installation/#1-verify-gpu-access","title":"1. Verify GPU Access","text":"<p>Run the following command inside the container to check GPU availability:</p> <pre><code>nvidia-smi\n</code></pre> <p>If the command fails or returns an error, verify that: - NVIDIA drivers are correctly installed. - Docker is configured for GPU passthrough.</p>"},{"location":"user-guide/installation/docker-installation/#2-check-environment-variables","title":"2. Check Environment Variables","text":"<p>Ensure that environment variables are correctly passed when downloading models:</p> <pre><code>echo $MODELS_URL\n</code></pre>"},{"location":"user-guide/installation/docker-installation/#3-inspect-docker-logs","title":"3. Inspect Docker Logs","text":"<p>If any issue arises, check the container logs:</p> <pre><code>docker logs &lt;container_id&gt;\n</code></pre> <p>Replace <code>&lt;container_id&gt;</code> with the actual container ID obtained using:</p> <pre><code>docker ps -a\n</code></pre>"},{"location":"user-guide/installation/docker-installation/#next-steps","title":"Next Steps","text":"<p>Once your Docker container is set up and running, proceed to download and configure the necessary AI models.</p> <p>Go to Model Download Guide</p>"},{"location":"user-guide/installation/docker-installation/#contact","title":"Contact","text":"<p>For additional support, visit Docker Hub, or reach out to the UniversaLabeler team.</p> <p>Now that installation is complete, you\u2019re all set to start using UniversaLabeler inside a containerized environment! \ud83d\ude80</p>"},{"location":"user-guide/installation/environment-setup/","title":"Environment Setup","text":""},{"location":"user-guide/installation/environment-setup/#introduction","title":"Introduction","text":"<p>Setting up a dedicated environment ensures UniversaLabeler runs smoothly without conflicting with other dependencies. We recommend using a virtual environment, such as venv or Conda, to isolate the project dependencies.</p>"},{"location":"user-guide/installation/environment-setup/#virtual-environment-options","title":"Virtual Environment Options","text":"<p>UniversaLabeler supports two primary environment configurations:</p> <ol> <li>Python Virtual Environment (venv) \u2013 Recommended for most users.</li> <li>Conda Environment \u2013 Preferred for advanced users and package compatibility management.</li> </ol>"},{"location":"user-guide/installation/environment-setup/#setting-up-a-python-virtual-environment-venv","title":"Setting Up a Python Virtual Environment (venv)","text":"<ol> <li> <p>Ensure Python 3.9+ is installed: <pre><code>python3 --version\n</code></pre>    If not installed, follow the Python installation guide.</p> </li> <li> <p>Create a virtual environment: <pre><code>python3 -m venv ul_env\n</code></pre></p> </li> <li> <p>Activate the virtual environment:</p> </li> <li>Linux/macOS: <pre><code>source ul_env/bin/activate\n</code></pre></li> <li> <p>Windows: <pre><code>ul_env\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Verify activation:    Running the following command should show <code>(ul_env)</code> at the start of the command line:    <pre><code>which python\n</code></pre></p> </li> <li> <p>Proceed to Package Installation.</p> </li> </ol>"},{"location":"user-guide/installation/environment-setup/#setting-up-a-conda-environment","title":"Setting Up a Conda Environment","text":"<ol> <li>Ensure Conda is installed:</li> <li> <p>If not installed, download it from Anaconda or install Miniconda:      <pre><code>curl -o miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash miniconda.sh\n</code></pre></p> </li> <li> <p>Create a new Conda environment: <pre><code>conda create --name ul_env python=3.9 -y\n</code></pre></p> </li> <li> <p>Activate the Conda environment: <pre><code>conda activate ul_env\n</code></pre></p> </li> <li> <p>Verify activation: <pre><code>which python\n</code></pre></p> </li> <li> <p>Proceed to Package Installation.</p> </li> </ol>"},{"location":"user-guide/installation/environment-setup/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Python not found: Ensure Python is installed and accessible in the system path.</li> <li>Virtual environment activation fails: Use the absolute path when running activation commands.</li> <li>Package conflicts in Conda: Try running <code>conda update --all</code> before installing additional dependencies.</li> </ul>"},{"location":"user-guide/installation/environment-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Package Installation \u2013 Install required dependencies.</li> <li>Model Download \u2013 Obtain pre-trained models for inference.</li> </ul>"},{"location":"user-guide/installation/models-download/","title":"Model Download","text":""},{"location":"user-guide/installation/models-download/#introduction","title":"Introduction","text":"<p>To utilize UniversaLabeler, pre-trained model checkpoints must be downloaded and placed in the appropriate directory. This section guides you through the steps for retrieving and setting up these files.</p>"},{"location":"user-guide/installation/models-download/#step-1-download-model-checkpoints","title":"Step 1: Download Model Checkpoints","text":""},{"location":"user-guide/installation/models-download/#option-1-automatic-download-recommended","title":"Option 1: Automatic Download (Recommended)","text":"<p>Run the provided script to automatically fetch the required models:</p> <pre><code>python setup/download_models.py mega.nz/file/AiFzBSDS#BqcKazpnYaS0GR4i2HqHCsenbowzr9KjeQQ9X2VPFHY --mega\n</code></pre> <p>This script will: - Download the necessary model weights. - Verify the integrity of each file. - Place them in the correct directory.</p>"},{"location":"user-guide/installation/models-download/#option-2-manual-download","title":"Option 2: Manual Download","text":"<p>If you prefer to download models manually, use the provided links:</p> <ul> <li>MEGA Repository</li> </ul> <p>After downloading, extract the files into the following directory:</p> <p><code>UniversaLabeler/common/weights</code></p> <p>Ensure Correct Placement</p> <p>Model files must be located in UniversaLabeler/common/weights/ to be detected properly.</p>"},{"location":"user-guide/installation/models-download/#step-2-verify-installation","title":"Step 2: Verify Installation","text":"<p>To check that the models were installed correctly, please visit the UniversaLabeler/common/weights:</p>"},{"location":"user-guide/installation/models-download/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage \u2013 Learn how to run an initial inference.</li> <li>Advanced Configuration \u2013 Customize model settings and priorities.</li> </ul>"},{"location":"user-guide/installation/overview/","title":"Installation Overview","text":""},{"location":"user-guide/installation/overview/#introduction","title":"Introduction","text":"<p>This section provides an overview of the installation process for UniversaLabeler. Multiple installation methods are available depending on your use case, including standard installation, Docker-based setup, and advanced configurations.</p>"},{"location":"user-guide/installation/overview/#installation-methods","title":"Installation Methods","text":""},{"location":"user-guide/installation/overview/#1-standard-installation","title":"1. Standard Installation","text":"<p>The standard installation method is recommended for most users. It involves setting up a virtual environment and installing dependencies using <code>pip</code>.</p> <ul> <li>Suitable for local development and deployment.</li> <li>Requires Python 3.9 or 3.10.</li> <li>Uses <code>pip</code> for package management.</li> </ul> <p>Go to Standard Installation Guide</p>"},{"location":"user-guide/installation/overview/#2-docker-installation","title":"2. Docker Installation","text":"<p>For users who prefer containerized environments, a pre-configured Docker image is available.</p> <ul> <li>Ensures compatibility with all dependencies.</li> <li>Recommended for cloud-based applications and isolated environments.</li> </ul> <p>Go to Docker Installation Guide</p>"},{"location":"user-guide/installation/overview/#3-advanced-installation-options","title":"3. Advanced Installation Options","text":"<p>Additional methods such as Google Colab, REST API, and Conda environments are under development.</p> <ul> <li>Suitable for specialized use cases.</li> <li>Contact support for early access.</li> </ul>"},{"location":"user-guide/installation/overview/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation, ensure that your system meets the following requirements:</p> <ul> <li>Operating System: Ubuntu 20.04+, Windows 10/11 (WSL recommended), NVIDIA DGX.</li> <li>GPU Support: NVIDIA CUDA-compatible GPU for optimal performance.</li> <li>Python Version: 3.9 or 3.10.</li> <li>Git Installed: Required for cloning the repository.</li> </ul> <p>Important Warning</p> <p>Running without a GPU may result in significantly slower performance.</p>"},{"location":"user-guide/installation/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Platform-Specific Installation \u2013 Configure your OS for installation.</li> <li>Environment Setup \u2013 Creating a virtual environment.</li> <li>Package Installation \u2013 Installing dependencies.</li> <li>Downloading Models \u2013 Obtaining required AI model weights.</li> </ul>"},{"location":"user-guide/installation/package-installation/","title":"Package Installation","text":""},{"location":"user-guide/installation/package-installation/#introduction","title":"Introduction","text":"<p>Once the virtual environment is set up, the next step is to install the necessary dependencies for UniversaLabeler. This ensures all required libraries are correctly configured for smooth operation.</p>"},{"location":"user-guide/installation/package-installation/#installing-required-packages","title":"Installing Required Packages","text":""},{"location":"user-guide/installation/package-installation/#step-1-ensure-the-virtual-environment-is-activated","title":"Step 1: Ensure the Virtual Environment is Activated","text":"<p>Before installing packages, ensure that your virtual environment is activated:</p> <ul> <li>For venv (Linux/macOS): <pre><code>source ul_env/bin/activate\n</code></pre></li> <li>For venv (Windows): <pre><code>ul_env\\Scripts\\activate\n</code></pre></li> <li>For Conda: <pre><code>conda activate ul_env\n</code></pre></li> </ul>"},{"location":"user-guide/installation/package-installation/#step-2-install-pytorch-manually","title":"Step 2: Install PyTorch Manually","text":"<p>Since PyTorch is not included in <code>requirements.txt</code>, install it manually based on your system configuration:</p> <ol> <li>Visit the official PyTorch installation guide.</li> <li>Choose the appropriate configuration based on your system (CUDA, CPU, etc.).</li> <li>Install using the provided command, for example:    <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></li> </ol>"},{"location":"user-guide/installation/package-installation/#verifying-the-pytorch-installation","title":"Verifying the pytorch Installation","text":"<p>Run the following command to check if everything is installed correctly:</p> <p><pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> Expected output: - <code>True</code> \u2192 CUDA (GPU) is available. - <code>False</code> \u2192 Troubleshoot it.</p> <p>Running on CPU - not recommended but possible in some cases\"</p> <p>TODO</p> <p>In the next release, PyTorch will be installed automatically as part of the <code>requirements.txt</code> dependencies.</p>"},{"location":"user-guide/installation/package-installation/#step-3-install-other-required-python-packages","title":"Step 3: Install Other Required Python Packages","text":"<p>Run the following command to install all additional dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Important Notice</p> <p>Ensure that your pip is up-to-date before installing dependencies to prevent compatibility issues: <pre><code>pip install --upgrade pip\n</code></pre></p>"},{"location":"user-guide/installation/package-installation/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Package installation failure: Ensure Python and pip are correctly installed.</li> <li>Dependency conflicts: Run <code>pip check</code> to identify issues.</li> <li>PyTorch GPU not detected: Ensure CUDA is installed and drivers are up to date.</li> </ul>"},{"location":"user-guide/installation/package-installation/#next-steps","title":"Next Steps","text":"<ul> <li>Model Download \u2013 Download and configure model checkpoints.</li> <li>Basic Usage \u2013 Running your first image processing pipeline.</li> </ul>"},{"location":"user-guide/installation/platforms/","title":"Supported Platforms","text":""},{"location":"user-guide/installation/platforms/#introduction","title":"Introduction","text":"<p>UniversaLabeler supports multiple platforms, ensuring flexibility across different operating systems and hardware configurations. This section provides installation guidelines tailored to specific environments.</p>"},{"location":"user-guide/installation/platforms/#system-requirements","title":"System Requirements","text":"<p>Before proceeding, ensure your system meets the minimum hardware and software requirements:</p> <ul> <li>Operating Systems:</li> <li>Ubuntu 20.04+ (Recommended for best performance)</li> <li>Windows 10/11 (WSL recommended for full compatibility)</li> <li> <p>NVIDIA DGX (for enterprise-level GPU acceleration)</p> </li> <li> <p>Hardware Requirements:</p> </li> <li>NVIDIA CUDA-compatible GPU (recommended for optimal performance)</li> <li>Minimum 16GB RAM (32GB recommended for large-scale models)</li> <li>At least 20GB of available storage</li> </ul> <p>Windows Users</p> <p>For Windows users, it is highly recommended to use Windows Subsystem for Linux (WSL2) for improved compatibility. Standard Windows installation may face issues with GPU support and dependencies.</p>"},{"location":"user-guide/installation/platforms/#platform-specific-installation-guides","title":"Platform-Specific Installation Guides","text":""},{"location":"user-guide/installation/platforms/#ubuntu-recommended","title":"Ubuntu (Recommended)","text":"<ol> <li>Ensure system packages are up-to-date: <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></li> <li>Install dependencies: <pre><code>sudo apt install python3 python3-venv python3-pip git -y\n</code></pre></li> <li>Ensure GPU drivers and CUDA are installed: <pre><code>nvidia-smi\n</code></pre>    If CUDA is not installed, follow NVIDIA\u2019s installation guide.</li> <li>Proceed to Environment Setup.</li> </ol>"},{"location":"user-guide/installation/platforms/#windows-using-wsl2","title":"Windows (Using WSL2)","text":"<ol> <li>Enable WSL2 (If not already enabled):    <pre><code>wsl --install\n</code></pre></li> <li>Install Ubuntu on WSL2:</li> <li>Open PowerShell and run:      <pre><code>wsl --install -d Ubuntu-20.04\n</code></pre></li> <li>Update system packages: <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></li> <li>Ensure GPU support with NVIDIA CUDA for WSL2:</li> <li>Follow Microsoft\u2019s WSL2 GPU guide.</li> <li>Proceed to Environment Setup.</li> </ol>"},{"location":"user-guide/installation/platforms/#nvidia-dgx-systems","title":"NVIDIA DGX Systems","text":"<ol> <li>Ensure NVIDIA drivers are installed: <pre><code>nvidia-smi\n</code></pre></li> </ol>"},{"location":"user-guide/installation/platforms/#next-steps","title":"Next Steps","text":"<ul> <li>Environment Setup \u2013 Setting up virtual environments.</li> <li>Package Installation \u2013 Installing required dependencies.</li> <li>Model Download \u2013 Obtaining model weights.</li> </ul>"},{"location":"user-guide/usage/basic-usage/","title":"Basic Usage","text":""},{"location":"user-guide/usage/basic-usage/#introduction","title":"Introduction","text":"<p>This section provides an overview of how to use UniversaLabeler (UL) and its core functionalities. After completing the installation and downloading the necessary models, you will be ready to utilize the powerful UL Detection, UL Segmentation, and Image Captioning components.</p> <p>Caution</p> <p>You are now using an advanced and powerful AI-driven labeling tool. Please ensure responsible and accurate usage to achieve the best results.</p>"},{"location":"user-guide/usage/basic-usage/#ul-detection","title":"UL Detection","text":"<p>ULDetection is responsible for detecting objects in images and returning bounding box (BBOX) results. The class supports multiple models, allowing users to process images using prompt-based detection, predefined classes, and zero-shot learning models.</p>"},{"location":"user-guide/usage/basic-usage/#data-classes","title":"Data Classes","text":"<p>Several data classes are available under the <code>common</code> directory to manage object detection.</p> <pre><code>class DetectionClassesName(Enum):\n   SmallVehicle = \"SmallVehicle\"  # Alfred\n   BigVehicle = \"BigVehicle\"      # Alfred\n   # ... More classes\n</code></pre> <pre><code>class ModelNameRegistryDetection(Enum):\n   DINO = \"DINO\"\n   YOLO_WORLD = \"YOLO_WORLD\"\n   # ... More models\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#uldetection-parameters","title":"ULDetection Parameters","text":"<p>ULDetection accepts multiple parameters upon initialization:</p> <ul> <li><code>image_input (str | np.ndarray)</code>: Path to the image or a NumPy array representation.</li> <li><code>detection_class (List[str])</code>: List of detection class names or prompts.</li> <li><code>class_priorities (Dict[str, int])</code>: Priorities for class selection during post-processing.</li> <li><code>model_priorities (Dict[str, int])</code>: Priorities for model selection in case of overlapping detections.</li> <li><code>use_nms (bool)</code>: Whether to apply Non-Maximum Suppression (NMS) to refine results.</li> <li><code>nms_advanced_params (dict)</code>: Advanced NMS settings for customization.</li> <li><code>filter_unwanted_classes (bool)</code>: If <code>True</code>, removes unwanted detected classes.</li> <li><code>model_names (List[str])</code>: List of models to be used for detection.</li> <li><code>trex_input_class_bbox (dict)</code>: Bounding box configuration for TREX reference-based detection.</li> <li><code>sahi_models_params (dict)</code>: Configuration for SAHI preprocessing algorithms.</li> </ul>"},{"location":"user-guide/usage/basic-usage/#example-creating-an-instance-of-uldetection","title":"Example: Creating an Instance of ULDetection","text":"<pre><code>image_path = \"data/tested_image/sample.jpeg\"\n\ndetection_classes = [\"window\", \"car\", \"person\"]\n\n# SAHI Model Parameters\nsahi_model_params = {\n   ModelNameRegistryDetection.YOLO_WORLD.value: {\n       'slice_dimensions': (256, 256),\n       'detection_conf_threshold': 0.7\n   }\n}\n\nul_detection = ULDetection(\n   image_input=image_path,\n   detection_class=detection_classes,\n   class_priorities={\"window\": 2, \"car\": 1},\n   model_priorities={ModelNameRegistryDetection.YOLO_WORLD.value: 2, ModelNameRegistryDetection.DINOX_DETECTION.value: 1},\n   use_nms=True,\n   model_names=[ModelNameRegistryDetection.YOLO_WORLD.value, ModelNameRegistryDetection.DINOX_DETECTION.value],\n   filter_unwanted_classes=True,\n   sahi_models_params=sahi_model_params\n)\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#processing-an-image","title":"Processing an Image","text":"<pre><code>nms_results, individual_results = ul_detection.process_image()\n</code></pre> <ul> <li><code>nms_results</code>: The refined detection results after applying NMS.</li> <li><code>individual_results</code>: Raw detection results from each individual model.</li> </ul> <p>To save the results, run:</p> <pre><code>output_directory = \"./output_results\"\nul_detection.save_results(individual_results, nms_results, output_directory)\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#ul-segmentation","title":"UL Segmentation","text":"<p>ULSegmentation performs pixel-wise segmentation of images using multiple models. It supports semantic segmentation, instance segmentation, and panoptic segmentation.</p>"},{"location":"user-guide/usage/basic-usage/#data-classes_1","title":"Data Classes","text":"<pre><code>class ModelNameRegistrySegmentation(Enum):\n   DINOX_SEGMENTATION = \"DINOX_SEGMENTATION\"\n   SAM = \"SAM\"\n   # ... More models\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#ulsegmentation-parameters","title":"ULSegmentation Parameters","text":"<ul> <li><code>image_input (str | np.ndarray)</code>: Path to the image or a NumPy array representation.</li> <li><code>segmentation_class (List[str])</code>: List of segmentation class names.</li> <li><code>class_priorities (Dict[str, int])</code>: Priorities for resolving overlapping segmentations.</li> <li><code>model_priorities (Dict[str, int])</code>: Model selection priorities in case of multiple segmentations.</li> <li><code>use_segselector (bool)</code>: Enables the SegSelector algorithm for result refinement.</li> <li><code>seg_selector_advanced_params (dict)</code>: Advanced SegSelector settings.</li> <li><code>sam2_predict_on_bbox (List[np.ndarray])</code>: Bounding boxes to pass to SAM2 for segmentation.</li> </ul>"},{"location":"user-guide/usage/basic-usage/#example-creating-an-instance-of-ulsegmentation","title":"Example: Creating an Instance of ULSegmentation","text":"<pre><code>image_path = \"data/street/sample.png\"\n\nsegmentation_classes = [\"car\", \"bus\"]\n\nbounding_boxes = [\n   np.array([468, 157, 518, 203]),\n   np.array([313, 138, 408, 256])\n]\n\nul_segmentation = ULSegmentation(\n   image_input=image_path,\n   segmentation_class=segmentation_classes,\n   model_names=[ModelNameRegistrySegmentation.SAM2.value, ModelNameRegistrySegmentation.DINOX_SEGMENTATION.value],\n   sam2_predict_on_bbox=bounding_boxes,\n   model_priorities={ModelNameRegistrySegmentation.SEEM.value: 5, ModelNameRegistrySegmentation.DINOX_SEGMENTATION.value: 4}\n)\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#processing-an-image_1","title":"Processing an Image","text":"<pre><code>formatted_result, individual_results = ul_segmentation.process_image()\n</code></pre> <ul> <li><code>formatted_result</code>: SegSelector-refined segmentation results.</li> <li><code>individual_results</code>: Raw segmentation results from each model.</li> </ul> <p>To save segmentation results, run:</p> <pre><code>ul_segmentation.save_results(individual_results, \"output_segmentation_results\")\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#image-captioning","title":"Image Captioning","text":"<p>The Image Captioning module provides image description, classification, and object analysis using large language models (LLMs).</p>"},{"location":"user-guide/usage/basic-usage/#example-functions","title":"Example Functions","text":"<pre><code># Check if an object is present in the image\ndoes_exist = ul_caption.set_prompt_get_does_is(\"bus\", threshold=0.30)\n</code></pre> <pre><code># Generate a detailed image description\ndescription = ul_caption.set_prompt_get_describe_image(detail=DetailLevel.HIGH)\n</code></pre> <pre><code># Get all detected classes with synonyms and possible objects\nclasses = ul_caption.set_prompt_get_all_classes_from_image(also_like_to_be=True, add_synonym=True)\n</code></pre>"},{"location":"user-guide/usage/basic-usage/#summary","title":"Summary","text":"<p>This guide covers the basic usage of UniversaLabeler for:</p> <ul> <li>UL Detection: Object detection using bounding boxes.</li> <li>UL Segmentation: Pixel-wise image segmentation.</li> <li>Image Captioning: Image classification and description.</li> </ul> <p>For more advanced configurations, check out the Advanced Usage section.</p>"},{"location":"user-guide/usage/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Model Integration \u2013 Extend with custom models.</li> <li>API Reference \u2013 Explore API functionalities.</li> </ul>"},{"location":"user-guide/usage/get-started-usage/","title":"Getting Started with UniversaLabeler","text":""},{"location":"user-guide/usage/get-started-usage/#project-scope","title":"Project Scope","text":"<p>After successfully installing the package, downloading all required models, and passing the startup tests, we can now proceed with using UniversaLabeler and its components.</p> <p>Powerful tool</p> <p>You are now using a powerful and complex tool as part of a large-scale project. Please be mindful and responsible while using it.</p>"},{"location":"user-guide/usage/get-started-usage/#understanding-the-system","title":"Understanding the System","text":"<p>To effectively utilize the project, it is important to grasp its core structure. You can refer to the development section for deeper insights into its architecture and design.</p>"},{"location":"user-guide/usage/get-started-usage/#the-ul-class","title":"The UL Class","text":"<p>At the heart of UniversaLabeler lies the <code>UL</code> class, which serves as the central interface for executing various labeling tasks. It provides a structured approach to handling object detection, segmentation, and captioning workflows.</p> <p>Within this core class, we have the following specialized implementations:</p> <ul> <li><code>ULDetection</code>: Handles bounding box-based object detection.</li> <li><code>ULSegmentation</code>: Manages segmentation tasks with semantic, instance, and panoptic segmentation support.</li> <li><code>ULCaptioning</code>: Supports image description, classification, and object labeling tasks.</li> </ul> <p>Each of these classes provides a seamless interface for interacting with different AI models while incorporating pre-processing and post-processing pipelines.</p>"},{"location":"user-guide/usage/get-started-usage/#how-it-works","title":"How It Works","text":"<p>Each <code>UL</code> implementation uses a Factory Interface that dynamically initializes models based on user preferences. Additionally, it integrates with algorithmic enhancement layers, such as:</p> <ul> <li>Pre-Processing Mechanisms: Optimizing input images before inference.</li> <li>Post-Processing Enhancements: Refining results using techniques like Non-Maximum Suppression (NMS).</li> <li>Captioning Pipelines: Utilizing LLM-based reasoning for contextual image interpretation.</li> </ul> <p>This modular approach ensures flexibility and configurability while maintaining a user-friendly interface.</p>"},{"location":"user-guide/usage/get-started-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Learn how to use UniversaLabeler with practical examples.</li> <li>UL Detection(ul-detection.md) - A deep dive into detection workflows.</li> <li>UL Segmentation - Explore segmentation capabilities.</li> <li>UL Captioning - Learn about AI-powered image captioning.</li> </ul>"}]}